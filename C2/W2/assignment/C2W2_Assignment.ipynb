{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuW-xg_bTsaF"
      },
      "source": [
        "# Week 2: Tackle Overfitting with Data Augmentation\n",
        "\n",
        "Welcome to this assignment! As in the previous week, you will be using the famous `cats vs dogs` dataset to train a model that can classify images of dogs from images of cats. For this, you will create your own Convolutional Neural Network in Tensorflow and leverage Keras' image preprocessing utilities, more so this time around since Keras provides excellent support for augmenting image data.\n",
        "\n",
        "You will also need to create the helper functions to move the images around the filesystem as you did last week, so if you need to refresh your memory with the `os` module be sure to take a look a the [docs](https://docs.python.org/3/library/os.html).\n",
        "\n",
        "Let's get started!"
      ],
      "id": "AuW-xg_bTsaF"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dn-6c02VmqiN",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "dn-6c02VmqiN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLTQd84RUs1j"
      },
      "source": [
        "Download the dataset from its original source by running the cell below. \n",
        "\n",
        "Note that the `zip` file that contains the images is unzipped under the `/tmp` directory."
      ],
      "id": "bLTQd84RUs1j"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3sd9dQWa23aj",
        "lines_to_next_cell": 2,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977c4f2a-a427-43aa-cb9e-4afb1980df2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-28 12:03:23--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 23.62.161.112, 2600:1407:1800:390::317f, 2600:1407:1800:39b::317f\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|23.62.161.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824887076 (787M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/cats-and-dogs.zip’\n",
            "\n",
            "/tmp/cats-and-dogs. 100%[===================>] 786.67M   141MB/s    in 5.4s    \n",
            "\n",
            "2022-12-28 12:03:29 (145 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824887076/824887076]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If the URL doesn't work, visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n",
        "# And right click on the 'Download Manually' link to get a new URL to the dataset\n",
        "\n",
        "# Note: This is a very large dataset and will take some time to download\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\" \\\n",
        "    -O \"/tmp/cats-and-dogs.zip\"\n",
        "\n",
        "local_zip = '/tmp/cats-and-dogs.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "id": "3sd9dQWa23aj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_HsUV9WVJHL"
      },
      "source": [
        "Now the images are stored within the `/tmp/PetImages` directory. There is a subdirectory for each class, so one for dogs and one for cats."
      ],
      "id": "e_HsUV9WVJHL"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DM851ZmN28J3",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6180352-b61b-4e64-d4c9-c934d0c6371f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 12500 images of dogs.\n",
            "There are 12500 images of cats.\n"
          ]
        }
      ],
      "source": [
        "source_path = '/tmp/PetImages'\n",
        "\n",
        "source_path_dogs = os.path.join(source_path, 'Dog')\n",
        "source_path_cats = os.path.join(source_path, 'Cat')\n",
        "\n",
        "# Deletes all non-image files (there are two .db files bundled into the dataset)\n",
        "!find /tmp/PetImages/ -type f ! -name \"*.jpg\" -exec rm {} +\n",
        "\n",
        "# os.listdir returns a list containing all files under the given path\n",
        "print(f\"There are {len(os.listdir(source_path_dogs))} images of dogs.\")\n",
        "print(f\"There are {len(os.listdir(source_path_cats))} images of cats.\")"
      ],
      "id": "DM851ZmN28J3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7dI86rmRGmC"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "There are 12500 images of dogs.\n",
        "There are 12500 images of cats.\n",
        "```"
      ],
      "id": "G7dI86rmRGmC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFbMliudNIjW"
      },
      "source": [
        "You will need a directory for cats-v-dogs, and subdirectories for training\n",
        "and validation. These in turn will need subdirectories for 'cats' and 'dogs'. To accomplish this, complete the `create_train_val_dirs` below:"
      ],
      "id": "iFbMliudNIjW"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "code",
        "id": "F-QkLjxpmyK2",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Define root directory\n",
        "root_dir = '/tmp/cats-v-dogs'\n",
        "\n",
        "# Empty directory to prevent FileExistsError is the function is run several times\n",
        "if os.path.exists(root_dir):\n",
        "  shutil.rmtree(root_dir)\n",
        "\n",
        "# GRADED FUNCTION: create_train_val_dirs\n",
        "def create_train_val_dirs(root_path):\n",
        "  \"\"\"\n",
        "  Creates directories for the train and test sets\n",
        "  \n",
        "  Args:\n",
        "    root_path (string) - the base directory path to create subdirectories from\n",
        "  \n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"  \n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # HINT:\n",
        "  # Use os.makedirs to create your directories with intermediate subdirectories\n",
        "  # Don't hardcode the paths. Use os.path.join to append the new directories to the root_path parameter\n",
        "\n",
        "  va = os.path.join(root_path, 'validation')\n",
        "  tr = os.path.join(root_path, 'training')\n",
        "  os.makedirs(va)\n",
        "  os.makedirs(tr)\n",
        "\n",
        "  vac = os.path.join(va, 'cats')\n",
        "  vad = os.path.join(va, 'dogs')\n",
        "  trc = os.path.join(tr, 'cats')\n",
        "  trd = os.path.join(tr, 'dogs')\n",
        "  os.makedirs(vac)\n",
        "  os.makedirs(vad)\n",
        "  os.makedirs(trc)\n",
        "  os.makedirs(trd)\n",
        "\n",
        "  pass\n",
        "  \n",
        "  ### END CODE HERE\n",
        "\n",
        "  \n",
        "try:\n",
        "  create_train_val_dirs(root_path=root_dir)\n",
        "except FileExistsError:\n",
        "  print(\"You should not be seeing this since the upper directory is removed beforehand\")"
      ],
      "id": "F-QkLjxpmyK2"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5dhtL344OK00",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e538bc-7891-4dd6-880f-fa647715a6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/cats-v-dogs/validation\n",
            "/tmp/cats-v-dogs/training\n",
            "/tmp/cats-v-dogs/validation/dogs\n",
            "/tmp/cats-v-dogs/validation/cats\n",
            "/tmp/cats-v-dogs/training/dogs\n",
            "/tmp/cats-v-dogs/training/cats\n"
          ]
        }
      ],
      "source": [
        "# Test your create_train_val_dirs function\n",
        "\n",
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ],
      "id": "5dhtL344OK00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7A0RK3IQsvg"
      },
      "source": [
        "**Expected Output (directory order might vary):**\n",
        "\n",
        "``` txt\n",
        "/tmp/cats-v-dogs/training\n",
        "/tmp/cats-v-dogs/validation\n",
        "/tmp/cats-v-dogs/training/cats\n",
        "/tmp/cats-v-dogs/training/dogs\n",
        "/tmp/cats-v-dogs/validation/cats\n",
        "/tmp/cats-v-dogs/validation/dogs\n",
        "\n",
        "```"
      ],
      "id": "D7A0RK3IQsvg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R93T7HdE5txZ"
      },
      "source": [
        "Code the `split_data` function which takes in the following arguments:\n",
        "- SOURCE_DIR: directory containing the files\n",
        "\n",
        "- TRAINING_DIR: directory that a portion of the files will be copied to (will be used for training)\n",
        "\n",
        "- VALIDATION_DIR: directory that a portion of the files will be copied to (will be used for validation)\n",
        "\n",
        "- SPLIT_SIZE: determines the portion of images used for training.\n",
        "\n",
        "The files should be randomized, so that the training set is a random sample of the files, and the validation set is made up of the remaining files.\n",
        "\n",
        "For example, if `SOURCE_DIR` is `PetImages/Cat`, and `SPLIT_SIZE` is .9 then 90% of the images in `PetImages/Cat` will be copied to the `TRAINING_DIR` directory\n",
        "and 10% of the images will be copied to the `VALIDATION_DIR` directory.\n",
        "\n",
        "All images should be checked before the copy, so if they have a zero file length, they will be omitted from the copying process. If this is the case then your function should print out a message such as `\"filename is zero length, so ignoring.\"`. **You should perform this check before the split so that only non-zero images are considered when doing the actual split.**\n",
        "\n",
        "\n",
        "Hints:\n",
        "\n",
        "- `os.listdir(DIRECTORY)` returns a list with the contents of that directory.\n",
        "\n",
        "- `os.path.getsize(PATH)` returns the size of the file\n",
        "\n",
        "- `copyfile(source, destination)` copies a file from source to destination\n",
        "\n",
        "- `random.sample(list, len(list))` shuffles a list"
      ],
      "id": "R93T7HdE5txZ"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "code",
        "id": "zvSODo0f9LaU",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: split_data\n",
        "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "\n",
        "  \"\"\"\n",
        "  Splits the data into train and test sets\n",
        "  \n",
        "  Args:\n",
        "    SOURCE_DIR (string): directory path containing the images\n",
        "    TRAINING_DIR (string): directory path to be used for training\n",
        "    VALIDATION_DIR (string): directory path to be used for validation\n",
        "    SPLIT_SIZE (float): proportion of the dataset to be used for training\n",
        "    \n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  ### START CODE HERE\n",
        "\n",
        "  listi = []\n",
        "  for images in os.listdir(SOURCE_DIR):\n",
        "    if(os.path.getsize(os.path.join(SOURCE_DIR, images))>0):\n",
        "      listi.append(images)\n",
        "    else:\n",
        "      print(f\"{images} is zero length, so ignoring.\")\n",
        "\n",
        "  random.sample(listi, len(listi))\n",
        "\n",
        "  for i,images in enumerate(listi) :\n",
        "    if((i+1)/len(listi)<=SPLIT_SIZE):\n",
        "      copyfile(os.path.join(SOURCE_DIR, images), os.path.join(TRAINING_DIR, images))\n",
        "    else:\n",
        "      copyfile(os.path.join(SOURCE_DIR, images), os.path.join(VALIDATION_DIR, images))\n",
        "\n",
        "  pass\n",
        "\n",
        "  ### END CODE HERE\n"
      ],
      "id": "zvSODo0f9LaU"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FlIdoUeX9S-9",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af44a7e7-4eb6-4fae-fbee-266ee646fc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "666.jpg is zero length, so ignoring.\n",
            "11702.jpg is zero length, so ignoring.\n",
            "\n",
            "\n",
            "Original cat's directory has 12500 images\n",
            "Original dog's directory has 12500 images\n",
            "\n",
            "There are 11249 images of cats for training\n",
            "There are 11249 images of dogs for training\n",
            "There are 1250 images of cats for validation\n",
            "There are 1250 images of dogs for validation\n"
          ]
        }
      ],
      "source": [
        "# Test your split_data function\n",
        "\n",
        "# Define paths\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "\n",
        "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
        "VALIDATION_DIR = \"/tmp/cats-v-dogs/validation/\"\n",
        "\n",
        "TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"cats/\")\n",
        "VALIDATION_CATS_DIR = os.path.join(VALIDATION_DIR, \"cats/\")\n",
        "\n",
        "TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, \"dogs/\")\n",
        "VALIDATION_DOGS_DIR = os.path.join(VALIDATION_DIR, \"dogs/\")\n",
        "\n",
        "# Empty directories in case you run this cell multiple times\n",
        "if len(os.listdir(TRAINING_CATS_DIR)) > 0:\n",
        "  for file in os.scandir(TRAINING_CATS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TRAINING_DOGS_DIR)) > 0:\n",
        "  for file in os.scandir(TRAINING_DOGS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(VALIDATION_CATS_DIR)) > 0:\n",
        "  for file in os.scandir(VALIDATION_CATS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(VALIDATION_DOGS_DIR)) > 0:\n",
        "  for file in os.scandir(VALIDATION_DOGS_DIR):\n",
        "    os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = .9\n",
        "\n",
        "# Run the function\n",
        "# NOTE: Messages about zero length images should be printed out\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, VALIDATION_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, VALIDATION_DOGS_DIR, split_size)\n",
        "\n",
        "# Your function should perform copies rather than moving images so original directories should contain unchanged images\n",
        "print(f\"\\n\\nOriginal cat's directory has {len(os.listdir(CAT_SOURCE_DIR))} images\")\n",
        "print(f\"Original dog's directory has {len(os.listdir(DOG_SOURCE_DIR))} images\\n\")\n",
        "\n",
        "# Training and validation splits. Check that the number of images matches the expected output.\n",
        "print(f\"There are {len(os.listdir(TRAINING_CATS_DIR))} images of cats for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_DOGS_DIR))} images of dogs for training\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_CATS_DIR))} images of cats for validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_DOGS_DIR))} images of dogs for validation\")"
      ],
      "id": "FlIdoUeX9S-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvskJNOFVSaz"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "666.jpg is zero length, so ignoring.\n",
        "11702.jpg is zero length, so ignoring.\n",
        "\n",
        "\n",
        "Original cat's directory has 12500 images\n",
        "Original dog's directory has 12500 images\n",
        "\n",
        "There are 11249 images of cats for training\n",
        "There are 11249 images of dogs for training\n",
        "There are 1250 images of cats for validation\n",
        "There are 1250 images of dogs for validation\n",
        "```"
      ],
      "id": "hvskJNOFVSaz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zil4QmOD_mXF"
      },
      "source": [
        "Now that you have successfully organized the data in a way that can be easily fed to Keras' `ImageDataGenerator`, it is time for you to code the generators that will yield batches of images, both for training and validation. For this, complete the `train_val_generators` function below.\n",
        "\n",
        "Something important to note is that the images in this dataset come in a variety of resolutions. Luckily, the `flow_from_directory` method allows you to standarize this by defining a tuple called `target_size` that will be used to convert each image to this target resolution. **For this exercise use a `target_size` of (150, 150)**."
      ],
      "id": "Zil4QmOD_mXF"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "code",
        "id": "fQrZfVgz4j2g",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: train_val_generators\n",
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  \"\"\"\n",
        "  Creates the training and validation data generators\n",
        "  \n",
        "  Args:\n",
        "    TRAINING_DIR (string): directory path containing the training images\n",
        "    VALIDATION_DIR (string): directory path containing the testing/validation images\n",
        "    \n",
        "  Returns:\n",
        "    train_generator, validation_generator - tuple containing the generators\n",
        "  \"\"\"\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the arguments to augment the images)\n",
        "  train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "                                     rotation_range=45,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     shear_range=0.3,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True,\n",
        "                                     fill_mode='nearest')\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size=100,\n",
        "                                                      class_mode='binary',\n",
        "                                                      target_size=(150, 150))\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "  validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                                batch_size=100,\n",
        "                                                                class_mode='binary',\n",
        "                                                                target_size=(150, 150))\n",
        "  ### END CODE HERE\n",
        "  return train_generator, validation_generator\n"
      ],
      "id": "fQrZfVgz4j2g"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qM7FxrjGiobD",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaea906b-ef4b-465c-8959-8c1baf4db7fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22498 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Test your generators\n",
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
      ],
      "id": "qM7FxrjGiobD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiPNmSfZjHwJ"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "Found 22498 images belonging to 2 classes.\n",
        "Found 2500 images belonging to 2 classes.\n",
        "```\n"
      ],
      "id": "tiPNmSfZjHwJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI3oEmyQCZoO"
      },
      "source": [
        "One last step before training is to define the architecture of the model that will be trained.\n",
        "\n",
        "Complete the `create_model` function below which should return a Keras' `Sequential` model.\n",
        "\n",
        "Aside from defining the architecture of the model, you should also compile it so make sure to use a `loss` function that is compatible with the `class_mode` you defined in the previous exercise, which should also be compatible with the output of your network. You can tell if they aren't compatible if you get an error during training.\n",
        "\n",
        "**Note that you should use at least 3 convolution layers to achieve the desired performance.**"
      ],
      "id": "TI3oEmyQCZoO"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "code",
        "id": "oDPK8tUB_O9e",
        "lines_to_next_cell": 2,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "def create_model():\n",
        "  # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "  # USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  model = tf.keras.models.Sequential([ \n",
        "      tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation='relu', input_shape=(150,150,3)),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "      tf.keras.layers.Conv2D(32, (3,3), strides=(1,1), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "      tf.keras.layers.Conv2D(64, (3,3), strides=(1,1), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "      tf.keras.layers.Conv2D(64, (3,3), strides=(1,1), activation='relu'),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(1024, activation='relu'),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  \n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy']) \n",
        "    \n",
        "  ### END CODE HERE\n",
        "\n",
        "  return model\n"
      ],
      "id": "oDPK8tUB_O9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMFNJZmTCZv6"
      },
      "source": [
        "Now it is time to train your model!\n",
        "\n",
        "Note: You can ignore the `UserWarning: Possibly corrupt EXIF data.` warnings."
      ],
      "id": "SMFNJZmTCZv6"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5qE1G6JB4fMn",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04472052-4497-47f2-f201-0f303378b6dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            " 98/225 [============>.................] - ETA: 1:28 - loss: 0.7110 - accuracy: 0.5215"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/PIL/TiffImagePlugin.py:767: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/PIL/TiffImagePlugin.py:767: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/PIL/TiffImagePlugin.py:767: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/PIL/TiffImagePlugin.py:767: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/PIL/TiffImagePlugin.py:767: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/PIL/TiffImagePlugin.py:767: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/PIL/TiffImagePlugin.py:767: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "225/225 [==============================] - 166s 720ms/step - loss: 0.6966 - accuracy: 0.5329 - val_loss: 0.6866 - val_accuracy: 0.5520\n",
            "Epoch 2/30\n",
            "225/225 [==============================] - 162s 718ms/step - loss: 0.6769 - accuracy: 0.5733 - val_loss: 0.6709 - val_accuracy: 0.5756\n",
            "Epoch 3/30\n",
            "225/225 [==============================] - 159s 708ms/step - loss: 0.6420 - accuracy: 0.6225 - val_loss: 0.5856 - val_accuracy: 0.6860\n",
            "Epoch 4/30\n",
            "225/225 [==============================] - 162s 718ms/step - loss: 0.5878 - accuracy: 0.6851 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
            "Epoch 5/30\n",
            "225/225 [==============================] - 159s 708ms/step - loss: 0.5612 - accuracy: 0.7084 - val_loss: 0.5604 - val_accuracy: 0.7000\n",
            "Epoch 6/30\n",
            "225/225 [==============================] - 162s 718ms/step - loss: 0.5363 - accuracy: 0.7276 - val_loss: 0.4659 - val_accuracy: 0.7880\n",
            "Epoch 7/30\n",
            "225/225 [==============================] - 159s 707ms/step - loss: 0.5093 - accuracy: 0.7503 - val_loss: 0.4260 - val_accuracy: 0.8108\n",
            "Epoch 8/30\n",
            "225/225 [==============================] - 162s 717ms/step - loss: 0.4900 - accuracy: 0.7634 - val_loss: 0.4625 - val_accuracy: 0.7680\n",
            "Epoch 9/30\n",
            "225/225 [==============================] - 161s 716ms/step - loss: 0.4705 - accuracy: 0.7744 - val_loss: 0.3875 - val_accuracy: 0.8336\n",
            "Epoch 10/30\n",
            "225/225 [==============================] - 160s 709ms/step - loss: 0.4476 - accuracy: 0.7898 - val_loss: 0.3872 - val_accuracy: 0.8264\n",
            "Epoch 11/30\n",
            "225/225 [==============================] - 161s 717ms/step - loss: 0.4268 - accuracy: 0.8026 - val_loss: 0.3952 - val_accuracy: 0.8296\n",
            "Epoch 12/30\n",
            "225/225 [==============================] - 160s 709ms/step - loss: 0.4064 - accuracy: 0.8132 - val_loss: 0.3724 - val_accuracy: 0.8348\n",
            "Epoch 13/30\n",
            "225/225 [==============================] - 164s 730ms/step - loss: 0.3972 - accuracy: 0.8224 - val_loss: 0.3423 - val_accuracy: 0.8520\n",
            "Epoch 14/30\n",
            "225/225 [==============================] - 159s 706ms/step - loss: 0.3761 - accuracy: 0.8304 - val_loss: 0.3243 - val_accuracy: 0.8636\n",
            "Epoch 15/30\n",
            "225/225 [==============================] - 160s 712ms/step - loss: 0.3682 - accuracy: 0.8355 - val_loss: 0.3478 - val_accuracy: 0.8488\n",
            "Epoch 16/30\n",
            "225/225 [==============================] - 160s 713ms/step - loss: 0.3547 - accuracy: 0.8440 - val_loss: 0.2992 - val_accuracy: 0.8764\n",
            "Epoch 17/30\n",
            "225/225 [==============================] - 159s 708ms/step - loss: 0.3351 - accuracy: 0.8555 - val_loss: 0.4003 - val_accuracy: 0.8208\n",
            "Epoch 18/30\n",
            "225/225 [==============================] - 161s 715ms/step - loss: 0.3295 - accuracy: 0.8545 - val_loss: 0.2916 - val_accuracy: 0.8728\n",
            "Epoch 19/30\n",
            "225/225 [==============================] - 159s 706ms/step - loss: 0.3179 - accuracy: 0.8623 - val_loss: 0.3346 - val_accuracy: 0.8544\n",
            "Epoch 20/30\n",
            "225/225 [==============================] - 160s 713ms/step - loss: 0.3060 - accuracy: 0.8646 - val_loss: 0.2847 - val_accuracy: 0.8764\n",
            "Epoch 21/30\n",
            "225/225 [==============================] - 159s 706ms/step - loss: 0.2964 - accuracy: 0.8723 - val_loss: 0.2786 - val_accuracy: 0.8892\n",
            "Epoch 22/30\n",
            "225/225 [==============================] - 161s 714ms/step - loss: 0.2976 - accuracy: 0.8689 - val_loss: 0.2496 - val_accuracy: 0.9000\n",
            "Epoch 23/30\n",
            "225/225 [==============================] - 159s 706ms/step - loss: 0.2900 - accuracy: 0.8715 - val_loss: 0.2459 - val_accuracy: 0.8960\n",
            "Epoch 24/30\n",
            "225/225 [==============================] - 160s 712ms/step - loss: 0.2806 - accuracy: 0.8820 - val_loss: 0.3140 - val_accuracy: 0.8596\n",
            "Epoch 25/30\n",
            "225/225 [==============================] - 160s 713ms/step - loss: 0.2738 - accuracy: 0.8811 - val_loss: 0.2475 - val_accuracy: 0.8948\n",
            "Epoch 26/30\n",
            "225/225 [==============================] - 159s 707ms/step - loss: 0.2715 - accuracy: 0.8815 - val_loss: 0.2418 - val_accuracy: 0.9012\n",
            "Epoch 27/30\n",
            "225/225 [==============================] - 160s 713ms/step - loss: 0.2674 - accuracy: 0.8858 - val_loss: 0.2377 - val_accuracy: 0.9024\n",
            "Epoch 28/30\n",
            "225/225 [==============================] - 159s 706ms/step - loss: 0.2561 - accuracy: 0.8915 - val_loss: 0.2349 - val_accuracy: 0.9024\n",
            "Epoch 29/30\n",
            "225/225 [==============================] - 160s 712ms/step - loss: 0.2673 - accuracy: 0.8847 - val_loss: 0.2345 - val_accuracy: 0.9036\n",
            "Epoch 30/30\n",
            "225/225 [==============================] - 159s 707ms/step - loss: 0.2519 - accuracy: 0.8923 - val_loss: 0.2372 - val_accuracy: 0.8992\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "# Note that this may take some time.\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator)"
      ],
      "id": "5qE1G6JB4fMn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGsaDMc-GMd4"
      },
      "source": [
        "Once training has finished, you can run the following cell to check the training and validation accuracy achieved at the end of each epoch.\n",
        "\n",
        "**To pass this assignment, your model should achieve a training and validation accuracy of at least 80% and the final testing accuracy should be either higher than the training one or have a 5% difference at maximum**. If your model didn't achieve these thresholds, try training again with a different model architecture, remember to use at least 3 convolutional layers or try tweaking the image augmentation process.\n",
        "\n",
        "You might wonder why the training threshold to pass this assignment is significantly lower compared to last week's assignment. Image augmentation does help with overfitting but usually this comes at the expense of requiring more training time. To keep the training time reasonable, the same number of epochs as in the previous assignment are kept. \n",
        "\n",
        "However, as an optional exercise you are encouraged to try training for more epochs and to achieve really good training and validation accuracies."
      ],
      "id": "VGsaDMc-GMd4"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=load_img(path, target_size=(150, 150))\n",
        "  \n",
        "  x=img_to_array(img)\n",
        "  x /= 255\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  \n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  print(classes[0])\n",
        "  \n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a dog\")\n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ],
      "metadata": {
        "id": "J20RjcAqWU1N",
        "outputId": "2b8864bb-e987-421e-c838-24682608c6f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "J20RjcAqWU1N",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1accccfe-e1b0-44c1-8a02-7823dbfbfe97\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1accccfe-e1b0-44c1-8a02-7823dbfbfe97\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cat.jpeg to cat (1).jpeg\n",
            "Saving cat1.jpeg to cat1 (1).jpeg\n",
            "Saving cat2.jpeg to cat2 (1).jpeg\n",
            "Saving cat3.jpeg to cat3 (1).jpeg\n",
            "Saving cat5.jpeg to cat5 (1).jpeg\n",
            "Saving cat6.jpeg to cat6 (1).jpeg\n",
            "Saving cat7.jpeg to cat7 (1).jpeg\n",
            "Saving cat8.jpeg to cat8 (1).jpeg\n",
            "Saving cats.jpeg to cats (1).jpeg\n",
            "Saving cqt4.jpeg to cqt4 (1).jpeg\n",
            "Saving dog1.jpeg to dog1 (1).jpeg\n",
            "Saving dog2.jpeg to dog2 (1).jpeg\n",
            "Saving dog3.jpeg to dog3 (1).jpeg\n",
            "Saving dog4.jpeg to dog4 (1).jpeg\n",
            "Saving dog5.jpeg to dog5 (1).jpeg\n",
            "Saving dog6.jpeg to dog6 (1).jpeg\n",
            "Saving dog7.jpeg to dog7 (1).jpeg\n",
            "Saving dog8.jpeg to dog8 (1).jpeg\n",
            "Saving dog9.jpeg to dog9 (1).jpeg\n",
            "Saving dog10.jpeg to dog10 (1).jpeg\n",
            "Saving dog11.jpeg to dog11 (1).jpeg\n",
            "Saving dog12.jpeg to dog12 (1).jpeg\n",
            "Saving dogs.jpg to dogs (1).jpg\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "[0.00946899]\n",
            "cat.jpeg is a cat\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "[0.70795083]\n",
            "cat1.jpeg is a dog\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[0.9567344]\n",
            "cat2.jpeg is a dog\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "[0.00206149]\n",
            "cat3.jpeg is a cat\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "[0.1462196]\n",
            "cat5.jpeg is a cat\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "[0.02057971]\n",
            "cat6.jpeg is a cat\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "[0.7777944]\n",
            "cat7.jpeg is a dog\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "[0.465337]\n",
            "cat8.jpeg is a cat\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "[3.147768e-05]\n",
            "cats.jpeg is a cat\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "[0.0001029]\n",
            "cqt4.jpeg is a cat\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "[0.9991974]\n",
            "dog1.jpeg is a dog\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "[0.37827465]\n",
            "dog2.jpeg is a cat\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "[0.97887236]\n",
            "dog3.jpeg is a dog\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[0.99888414]\n",
            "dog4.jpeg is a dog\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[0.97481877]\n",
            "dog5.jpeg is a dog\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "[0.99853146]\n",
            "dog6.jpeg is a dog\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "[0.9990477]\n",
            "dog7.jpeg is a dog\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "[0.74472183]\n",
            "dog8.jpeg is a dog\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "[0.98293453]\n",
            "dog9.jpeg is a dog\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "[0.9998745]\n",
            "dog10.jpeg is a dog\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "[0.8304869]\n",
            "dog11.jpeg is a dog\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "[0.9998673]\n",
            "dog12.jpeg is a dog\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "[0.1895063]\n",
            "dogs.jpg is a cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MWZrJN4-65RC",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "fe0e9cd0-e9ff-45dd-e28d-b4fe65dbc888"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "plt.show()"
      ],
      "id": "MWZrJN4-65RC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYIaqsN2pav6"
      },
      "source": [
        "You will probably encounter that the model is overfitting, which means that it is doing a great job at classifying the images in the training set but struggles with new data. This is perfectly fine and you will learn how to mitigate this issue in the upcomming week.\n",
        "\n",
        "Before closing the assignment, be sure to also download the `history.pkl` file which contains the information of the training history of your model. You can download this file by running the cell below:"
      ],
      "id": "NYIaqsN2pav6"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yWcrc9nZTsHj",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6672fdc1-d8bc-4235-8e70-47a864046374"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1193465a-59ff-4b68-afae-f86b70973a3a\", \"history_augmented.pkl\", 616)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history_augmented.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history_augmented.pkl')\n",
        "\n",
        "download_history()"
      ],
      "id": "yWcrc9nZTsHj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEj7UVe0OgMq"
      },
      "source": [
        "You will also need to submit this notebook for grading. To download it, click on the `File` tab in the upper left corner of the screen then click on `Download` -> `Download .ipynb`. You can name it anything you want as long as it is a valid `.ipynb` (jupyter notebook) file."
      ],
      "id": "yEj7UVe0OgMq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joAaZSWWpbOI"
      },
      "source": [
        "**Congratulations on finishing this week's assignment!**\n",
        "\n",
        "You have successfully implemented a convolutional neural network that classifies images of cats and dogs, along with the helper functions needed to pre-process the images!\n",
        "\n",
        "**Keep it up!**"
      ],
      "id": "joAaZSWWpbOI"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}